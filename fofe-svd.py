from pyspark import SparkContext, SparkConf
from copy import deepcopy
from scipy.sparse import coo_matrix, csr_matrix
from sklearn.decomposition import TruncatedSVD
import codecs
import argparse
import logging
import cPickle
import numpy

logger = logging.getLogger( __name__ )



def loadVocab(filename):
    word2idx, n = {}, 0
    with codecs.open(filename, 'rb', 'utf8') as fp:
        for line in fp:
            word2idx[line.strip().split()[0]] = n
            n += 1
    return word2idx



if __name__ == '__main__':
    logging.basicConfig( 
        format='%(asctime)s : %(levelname)s : %(message)s', 
        level=logging.INFO 
    )


    parser = argparse.ArgumentParser()
    parser.add_argument('data', type=str, help='training data in a single file, one sentence per line' )
    parser.add_argument('vocab', type=str, help='generated by word-count.py' )
    parser.add_argument('outbase', type=str, 
                        help='basename of sparse martrix "outbase-ppmi" after PPMI and ' + 
                             'embedding "outbase-embed" after TruncatedSVD')
    parser.add_argument('--embed_dim', type=int, default=256, help='dimension of the desired word embedding')
    parser.add_argument('--alpha', type=float, default=0.7, help='forgetting factor')
    parser.add_argument('--bidirectoinal', default=False, action='store_true')
    parser.add_argument('--n_iter', type=int, default=16, help='#iterations of TruncatedSVD')
    args = parser.parse_args()
    logger.info(args)

    conf = SparkConf().setAppName('fofe-svd')
    conf.set('spark.driver.maxResultSize', '4g')
    conf.set('spark.driver.memory', '8g')
    conf.set('spark.executor.memory', '2g')
    sc = SparkContext(conf=conf)
    
    word2idx = loadVocab(args.vocab)
    unk = word2idx['<unk>']
    n_word = len(word2idx)
    word2idx = sc.broadcast(word2idx)


    ####################################################
    # Accumulate weighted occurrence
    # http://www.aclweb.org/anthology/D/D17/D17-1031.pdf

    def __fofeContext(numeric):
        res = [{}]
        for i in xrange(len(numeric) - 1):
            r = deepcopy(res[-1])
            for k in r:
                r[k] = r[k] * args.alpha
            if numeric[i] in r:
                r[numeric[i]] += 1.0
            else:
                r[numeric[i]] = 1.0
            res.append(r)
        return zip(numeric, res)


    def fofeContext(line):
        tokens = [u'<s>']
        tokens.extend(line.strip().split())
        tokens.append(u'</s>')
        numeric = [word2idx.value.get(w, unk) for w in tokens]

        if len(numeric) > 2:
            if not args.bidirectoinal:
                return __fofeContext(numeric)
            else:
                left = __fofeContext(numeric)
                right = __fofeContext(reversed(numeric))
                right.reverse()
                for k in right:
                    left[k + n_word] = right[k]
                return left
        else:
            return []


    def accmulateContext(context1, context2):
        for k in context2:
            if k in context1:
                context1[k] = context1[k] + context2[k]
            else:
                context1[k] = context2[k]
        return context1


    def context2coo(context):
        w, c = context
        return [(w, k, c[k]) for k in c] 


    context = sc.textFile(args.data).flatMap(fofeContext).reduceByKey(accmulateContext)

    # the raw fofe matrix in coo (triples)
    coo = context.flatMap(context2coo)


    ################################################
    # Compute PPMI
    # https://web.stanford.edu/~jurafsky/slp3/15.pdf

    # P(w)
    rowSum = coo.map(lambda t: (t[0], t[2])).reduceByKey(lambda v1, v2: v1 + v2).collect()
    rowSumArr = [0] * n_word
    for i, cnt in rowSum:
        rowSumArr[i] = cnt
    
    # P(c)
    colSum = coo.map(lambda t: (t[1], t[2])).reduceByKey(lambda v1, v2: v1 + v2).collect()
    colSumArr = [0] * n_word
    for i, cnt in colSum:
        colSumArr[i] = cnt

    totalFreq = sum(rowSumArr)
    rowSum = sc.broadcast(rowSumArr)
    colSum = sc.broadcast(colSumArr)


    def ppmi(cell):
        w, c, v = cell
        pmi = numpy.log2(v * totalFreq / (rowSum.value[w] * colSum.value[c]))
        return (w, c, max(pmi, 0))


    coo = coo.map(ppmi).filter(lambda t: t[2] > 0).collect()
    coo.sort()
    coo = numpy.asarray(coo)
    row = coo[:,0].astype(numpy.int64)
    col = coo[:,1].astype(numpy.int64)
    val = coo[:,2].astype(numpy.float32)
    fofe = coo_matrix(
        (val, (row, col)), 
        shape=(n_word, n_word * (int(args.bidirectoinal) + 1))
    ).astype(numpy.float32)
    fofe = fofe.tocsr()
    logger.info('fofe matrix after ppmi has %d non-zero elements' % fofe.nnz)
    numpy.save('%s-ppmi' % args.outbase, fofe)

    svd = TruncatedSVD(n_components=args.embed_dim, n_iter=16)
    embed = svd.fit_transform(fofe)
    numpy.save('%s-embed' % args.outbase, embed)
